{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing all the necessary packages is done in the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#import all the necessary packages.\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import math\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity  \n",
    "from sklearn.metrics import pairwise_distances\n",
    "from matplotlib import gridspec\n",
    "from scipy.sparse import hstack\n",
    "import plotly\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.graph_objs import Scatter, Layout\n",
    "\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-51e79cd433a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbottleneck_features_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'16k_data_cnn_features.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0masins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'16k_data_cnn_feature_asins.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m                 return format.read_array(fid, allow_pickle=allow_pickle,\n\u001b[0;32m--> 419\u001b[0;31m                                          pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[1;32m    420\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m             \u001b[0;31m# Try a pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    649\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m             \u001b[0;31m# We can use the fast fromfile() function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;31m# This is not a real file. We have to read it the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bottleneck_features_train = np.load('16k_data_cnn_features.npy')\n",
    "asins = np.load('16k_data_cnn_feature_asins.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_pickle('pickels/16k_apperal_data_preprocessed')\n",
    "df_asins = list(data['asin'])\n",
    "asins = list(asins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some of the brand values are empty. \n",
    "# Need to replace Null with string \"NULL\"\n",
    "data['brand'].fillna(value=\"Not given\", inplace=True )\n",
    "\n",
    "# replace spaces with hypen\n",
    "brands = [x.replace(\" \", \"-\") for x in data['brand'].values]\n",
    "types = [x.replace(\" \", \"-\") for x in data['product_type_name'].values]\n",
    "colors = [x.replace(\" \", \"-\") for x in data['color'].values]\n",
    "\n",
    "brand_vectorizer = CountVectorizer()\n",
    "brand_features = brand_vectorizer.fit_transform(brands)\n",
    "\n",
    "type_vectorizer = CountVectorizer()\n",
    "type_features = type_vectorizer.fit_transform(types)\n",
    "\n",
    "color_vectorizer = CountVectorizer()\n",
    "color_features = color_vectorizer.fit_transform(colors)\n",
    "\n",
    "extra_features = hstack((brand_features, type_features, color_features)).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('word2vec_model', 'rb') as handle:\n",
    "    model = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word_vec(sentence, doc_id, m_name):\n",
    "    # sentence : title of the apparel\n",
    "    # doc_id: document id in our corpus\n",
    "    # m_name: model information it will take two values\n",
    "        # if  m_name == 'avg', we will append the model[i], w2v representation of word i\n",
    "        # if m_name == 'weighted', we will multiply each w2v[word] with the idf(word)\n",
    "    vec = []\n",
    "    for i in sentence.split():\n",
    "        if i in vocab:\n",
    "            if m_name == 'weighted' and i in  idf_title_vectorizer.vocabulary_:\n",
    "                vec.append(idf_title_features[doc_id, idf_title_vectorizer.vocabulary_[i]] * model[i])\n",
    "            elif m_name == 'avg':\n",
    "                vec.append(model[i])\n",
    "        else:\n",
    "            # if the word in our courpus is not there in the google word2vec corpus, we are just ignoring it\n",
    "            vec.append(np.zeros(shape=(300,)))\n",
    "    # we will return a numpy array of shape (#number of words in title * 300 ) 300 = len(w2v_model[word])\n",
    "    # each row represents the word2vec representation of each word (weighted/avg) in given sentance \n",
    "    return  np.array(vec)\n",
    "\n",
    "def get_distance(vec1, vec2):\n",
    "    # vec1 = np.array(#number_of_words_title1 * 300), each row is a vector of length 300 corresponds to each word in give title\n",
    "    # vec2 = np.array(#number_of_words_title2 * 300), each row is a vector of length 300 corresponds to each word in give title\n",
    "    \n",
    "    final_dist = []\n",
    "    # for each vector in vec1 we caluclate the distance(euclidean) to all vectors in vec2\n",
    "    for i in vec1:\n",
    "        dist = []\n",
    "        for j in vec2:\n",
    "            # np.linalg.norm(i-j) will result the euclidean distance between vectors i, j\n",
    "            dist.append(np.linalg.norm(i-j))\n",
    "        final_dist.append(np.array(dist))\n",
    "    # final_dist = np.array(#number of words in title1 * #number of words in title2)\n",
    "    # final_dist[i,j] = euclidean distance between vectors i, j\n",
    "    return np.array(final_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vocab = stores all the words that are there in google w2v model\n",
    "# vocab = model.wv.vocab.keys() # if you are using Google word2Vec\n",
    "\n",
    "vocab = model.keys()\n",
    "# this function will add the vectors of each word and returns the avg vector of given sentance\n",
    "def build_avg_vec(sentence, num_features, doc_id, m_name):\n",
    "    # sentace: its title of the apparel\n",
    "    # num_features: the lenght of word2vec vector, its values = 300\n",
    "    # m_name: model information it will take two values\n",
    "        # if  m_name == 'avg', we will append the model[i], w2v representation of word i\n",
    "        # if m_name == 'weighted', we will multiply each w2v[word] with the idf(word)\n",
    "\n",
    "    featureVec = np.zeros((num_features,), dtype=\"float32\")\n",
    "    # we will intialize a vector of size 300 with all zeros\n",
    "    # we add each word2vec(wordi) to this fetureVec\n",
    "    nwords = 0\n",
    "    \n",
    "    for word in sentence.split():\n",
    "        nwords += 1\n",
    "        if word in vocab:\n",
    "            if m_name == 'weighted' and word in  idf_title_vectorizer.vocabulary_:\n",
    "                featureVec = np.add(featureVec, idf_title_features[doc_id, idf_title_vectorizer.vocabulary_[word]] * model[word])\n",
    "            elif m_name == 'avg':\n",
    "                featureVec = np.add(featureVec, model[word])\n",
    "    if(nwords>0):\n",
    "        featureVec = np.divide(featureVec, nwords)\n",
    "    # returns the avg vector of given sentance, its of shape (1, 300)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idf_title_vectorizer = CountVectorizer()\n",
    "idf_title_features = idf_title_vectorizer.fit_transform(data['title'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def n_containing(word):\n",
    "    # return the number of documents which had the given word\n",
    "    return sum(1 for blob in data['title'] if word in blob.split())\n",
    "\n",
    "def idf(word):\n",
    "    # idf = log(#number of docs / #number of docs which had the given word)\n",
    "    return math.log(data.shape[0] / (n_containing(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we need to convert the values into float\n",
    "idf_title_features  = idf_title_features.astype(np.float)\n",
    "\n",
    "for i in idf_title_vectorizer.vocabulary_.keys():\n",
    "    # for every word in whole corpus we will find its idf value\n",
    "    idf_val = idf(i)\n",
    "    \n",
    "    # to calculate idf_title_features we need to replace the count values with the idf values of the word\n",
    "    # idf_title_features[:, idf_title_vectorizer.vocabulary_[i]].nonzero()[0] will return all documents in which the word i present\n",
    "    for j in idf_title_features[:, idf_title_vectorizer.vocabulary_[i]].nonzero()[0]:\n",
    "        \n",
    "        # we replace the count values of word i in document j with  idf_value of word i \n",
    "        # idf_title_features[doc_id, index_of_word_in_courpus] = idf value of word\n",
    "        idf_title_features[j,idf_title_vectorizer.vocabulary_[i]] = idf_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_id = 0\n",
    "w2v_title_weight = []\n",
    "# for every title we build a weighted vector representation\n",
    "for i in data['title']:\n",
    "    w2v_title_weight.append(build_avg_vec(i, 300, doc_id,'weighted'))\n",
    "    doc_id += 1\n",
    "# w2v_title = np.array(# number of doc in courpus * 300), each row corresponds to a doc \n",
    "w2v_title_weight = np.array(w2v_title_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def idf_w2v_brand_image(doc_id, w1, w2, w3, num_results):\n",
    "    # doc_id: apparel's id in given corpus\n",
    "    # w1: weight for  w2v features\n",
    "    # w2: weight for brand and color features\n",
    "\n",
    "    # pairwise_dist will store the distance from given input apparel to all remaining apparels\n",
    "    # the metric we used here is cosine, the coside distance is mesured as K(X, Y) = <X, Y> / (||X||*||Y||)\n",
    "    # http://scikit-learn.org/stable/modules/metrics.html#cosine-similarity\n",
    "    idf_w2v_dist  = pairwise_distances(w2v_title_weight, w2v_title_weight[doc_id].reshape(1,-1))\n",
    "    ex_feat_dist = pairwise_distances(extra_features, extra_features[doc_id])\n",
    "    doc_id = asins.index(df_asins[doc_id])\n",
    "    img_dist = pairwise_distances(bottleneck_features_train, bottleneck_features_train[doc_id].reshape(1,-1))\n",
    "    pairwise_dist   = (w1 * idf_w2v_dist +  w2 * ex_feat_dist + w3 * img_dist)/float(w1 + w2 + w3)\n",
    "\n",
    "    # np.argsort will return indices of 9 smallest distances\n",
    "    indices = np.argsort(pairwise_dist.flatten())[0:num_results]\n",
    "    #pdists will store the 9 smallest distances\n",
    "    pdists  = np.sort(pairwise_dist.flatten())[0:num_results]\n",
    "\n",
    "    #data frame indices of the 9 smallest distace's\n",
    "    df_indices = list(data.index[indices])\n",
    "    \n",
    "\n",
    "    for i in range(len(indices)):\n",
    "        rows = data[['medium_image_url','title']].loc[data['asin']==asins[indices[i]]]\n",
    "        for indx, row in rows.iterrows():\n",
    "            display(Image(url=row['medium_image_url'], embed=True))\n",
    "        print('ASIN :',data['asin'].loc[df_indices[i]])\n",
    "        print('Brand :',data['brand'].loc[df_indices[i]])\n",
    "        print('Product Title: ', row['title'])\n",
    "        print('euclidean distance from input :', pdists[i])\n",
    "        print('='*125)\n",
    "\n",
    "idf_w2v_brand_image(12566, 5, 5, 5, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
